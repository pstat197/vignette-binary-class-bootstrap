---
title: "Application of Bootstrap Resampling on Student Mental Health Data"
subtitle: "Bootstrapping for Binary Classification Models"
author: "Luke Fields, Thomas Shi, Lily Li, Dingan Jiang"
published-title: "Updated"
editor: visual
format:
  html:
    code-fold: true
    code-tools: true
---

```{r, results="hide"}
# We will be using the tidyverse models for this vignette
# We will also use the boot library for our bootstrapping
library(boot) 
library(tidyverse)
library(tidymodels)
```

## Importance of Mental Health

Before we delve into the specific code and models we implemented to describe the importance of bootstrapping, we probably should talk about the importance of our data set. We all feel quite passionately about the importance of mental health within UCSB students, and what better way to gain more insight on such a critical topic than to analyze it with Data Science techniques. Certain students may be experiencing untreated mental health problems without even knowing, and their friends, family, and especially self might want to be aware of this so they can improve their condition. How awesome would it be if we could leverage machine learning and statistical models to provide some positive assistance to the mental health issue in today's college students? That is what we aim to do in this vignette, but there lies an issue: we did not have a lot of data.

![](images/paste-4835C77D.png){width="440"}

[Image 1 Source](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.theguardian.com%2Fsociety%2F2019%2Fsep%2F27%2Fanxiety-mental-breakdowns-depression-uk-students&psig=AOvVaw31twejv8dhQag9lEZk4CmH&ust=1670457602735000&source=images&cd=vfe&ved=0CA8QjRxqFwoTCICR9tCZ5vsCFQAAAAAdAAAAABAE)

## Prepping the Bootstrap

After searching the web, we settled on a [Student Mental Health](https://www.kaggle.com/datasets/shariful07/student-mental-health) data set from Kaggle that surveyed 100 University students and their mental health conditions. Ideally, we would want more observations and data points than this, but the lack of such is what is leading us to performing a bootstrap. We took this raw data set and factorized all of the categorical variables (including our responses), removed the survey time stamps, and converted `year` to a numeric form as some data cleaning.

```{r}
student_mh <- read.csv("https://raw.githubusercontent.com/pstat197/vignette-binary-class-bootstrap/main/data/student_mh_clean.csv")

student_mh <- student_mh %>%
  mutate(Married = factor(Married, levels = c("Yes", "No")),
         Depression = factor(Depression, levels = c("Yes", "No")),
         Anxiety = factor(Anxiety, levels = c("Yes", "No")),
         Panic_Attack = factor(Panic_Attack, levels = c("Yes", "No")),
         Seek_Help = factor(Seek_Help, levels = c("Yes", "No")),)

student_mh %>%
  head()  # Let's take a peak at our dataset
```

![](images/paste-32C62A3A.png){width="236"}

[Image 2 Source](https://thehustle.co/wp-content/uploads/2021/12/header-1.gif)

Now, we get to our actual goal of this project. Typically, bootstrapping is performed as a method that takes many different random samples from one sample as a whole when not enough data is available. That original sample is treated as the population in this scenario, and our new samples are each treated as individual ones from this "population". This is usually done to obtain a certain estimate with a higher degree of confidence. For example, if we only had a sample of 1000 students but wanted to measure the mean `Age` of students in a 30,000+ student University, then bootstrapping that original 1000 student sample over and over can provide us with more detail of how things tend to settle at a certain estimate. Since we have a small data set, we can not sample without replacement since this would decrease our population size for subsequent samples and increase bias. A good visualization of the overall bootstrap process can be seen below.

![](images/paste-321100E3.png){width="491"}

But, this is where the fun part comes in, which is the objective of our project. As previously stated, most bootstrapping is done to obtain an estimate or confidence interval of a certain parameter, like *median* or *maximum*. Our group, on the other hand, wanted to demonstrate a bootstrapped estimate of **accuracy**; in this case binary classification accuracy that measures how well our model was able to correctly predict a student having depression or anxiety. Building a single train/test model on a sample with only 100 observations is not going to yield deployable results, but perhaps if our model was consistently working well on the bootstrapping methods, it could be more implementable among college kids. A diagram of how our specific bootstrapping process will be performed can be seen below.

![Visualization of Our Bootstrap Process](images/bootstrap-pre-viz.jpg){width="477"}

## Exploratory Data Analysis

As with any Data Science project, especially ones where bootstrapping ins involved, some EDA is required to analyze certain relationships among variables.

![](images/anxiety.png)

The graph shows that the majority of students have GPA between 3.00 and 4.00. Students who respond that they suffer from anxiety do not have a low gap. As a result, we can infer that there isn't much of a connection between anxiety and a high or low GPA.

![](images/depression.png)

Students who reported having depression also revealed the same result. These GPA-high students experience depression as well. We can suggest that there is also no strong relationship between depression and GPAs. Other elements, such as interactions with family or friends, the stress of maintaining a student budget and living expenses, and the pressure of looking for internships and jobs, can contribute to depression and anxiety.

![](images/depression_age.png)\
![](images/anxiety_age-01.png)

Two graphs above show density distribution by age for anxiety and depression. Graphs capture the same pattern as a U shape, and the number of people aged 18 and 24 indicating anxiety and depression are the most. There is a observation that could account for this phenomenon. Freshmen may feel uneasy in their new environment because they are unfamiliar with university policies and programs.

![![](images/year2.png)](images/year1.png)

School year frequency also shows the same rule for depression and anxiety. The number of students replying to have depression and anxiety increases from year one and raise to peaks in year three. ​​Finding a full-time job and applying to graduate programs may put pressure on junior students. This may explain why junior students have the largest group of students having depression or anxiety.

Below is the correlation plot between each variable

![](images/corrplot.png)

## Example of our Model

Before we bootstrap across 1000 different samples, it might be cool to see how our model works. Take Group 9 Member Luke for example. He is a 21 year old male, fourth year, statistics major with a GPA ranging from 3.50-4.00. Along with the rest of the predictors, does our model predict that he is depressed?

![](images/luke_ex_pic.jpeg){width="374"}

```{r}
set.seed(27)
luke_info <- c("Male", 21, "Statistics", 4, "3.50 - 4.00", "No", "No", "Yes", "Yes", "No")

student_mh_w_luke <- rbind(student_mh, luke_info)

dep_split <- initial_split(student_mh_w_luke, prop = 0.7, strata = Depression)
  dep_train <- training(dep_split)
  dep_test <- testing(dep_split)
  
dep_recipe <- recipe(Depression ~ ., dep_train) %>%
  step_rm(Major) %>%
  step_rm(Anxiety) %>%
  step_dummy(all_nominal_predictors())

log_reg <- logistic_reg() %>% 
    set_engine("glm") %>% 
    set_mode("classification")

dep_wf <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(dep_recipe)

fit <- fit(dep_wf, dep_train)

pred_dep <- predict(fit, new_data = dep_test, type = "class") %>% 
  bind_cols(dep_test) %>%
  rename(Pred_Dep = .pred_class)

pred_dep %>%
  tail(n = 1)
```

No, it predicts that he is not depressed! Maybe our model has some sort of hidden image recognition because of that smile! All jokes aside, this is just one student with one set of information, and our model obviously needs a lot more if the bootstrap is going to work to the desired effect.

## Results and Confidence Intervals

Depression code chunk

```{r}
# acc_dep <- function(formula, data, i) {
#   data_dep <- data[i,]
#   dep_split <- initial_split(data_dep, prop = 0.7, strata = Depression)
#   dep_train <- training(dep_split)
#   dep_test <- testing(dep_split)
#   
#   dep_recipe <- recipe(Depression ~ ., dep_train) %>%
#     step_rm(Major) %>%
#     step_rm(Anxiety) %>%
#     step_dummy(all_nominal_predictors())
# 
#   dep_wf <- workflow() %>% 
#     add_model(log_reg) %>% 
#     add_recipe(dep_recipe)
# 
#   fit <- fit(dep_wf, dep_train)
# 
#   pred_dep <- predict(fit, new_data = dep_test, type = "class") %>% 
#     bind_cols(dep_test) 
# 
#   acc_dep <- pred_dep %>% accuracy(truth = Depression, estimate = .pred_class)
#   
#   return (acc_dep[3] %>% as.numeric())
# }
# 
# dep_results <- boot(data=student_mh, statistic=acc_dep,R=1000, formula=Depression ~ .)
# 
# boot.ci(dep_results, type="bca")
```

Anxiety code chunk

```{r}
# acc_anx <- function(formula, data, i){
#   data_anx <- data[i,]
#   anx_split <- initial_split(data_anx, prop = 0.7, strata = Anxiety)
#   anx_train <- training(anx_split)
#   anx_test <- testing(anx_split)
#   
#   anx_recipe <- recipe(Anxiety ~ ., anx_train) %>%
#     step_rm(Major) %>%
#     step_rm(Depression) %>%
#     step_dummy(all_nominal_predictors())
# 
#   anx_wf <- workflow() %>% 
#     add_model(log_reg) %>% 
#     add_recipe(anx_recipe)
# 
#   fit <- fit(anx_wf, anx_train)
# 
#   pred_anx <- predict(fit, new_data = anx_test, type = "class") %>% 
#     bind_cols(anx_test) 
# 
#   acc_anx <- pred_anx %>% accuracy(truth = Anxiety, estimate = .pred_class)
#   
#   return (acc_anx[3] %>% as.numeric())
# }
# 
# anx_results <- boot(data=student_mh, statistic=acc_anx,R=1000, formula=Anxiety ~ .)
# 
# boot.ci(anx_results, type="bca")
```

We are using 1000 bootstrap samples to provide a 95% confidence interval of prediction accuracies of students with mental illness. By providing a confidence interval, we get an averaged prediction accuracy from 1000 samples that has less bias, and we are able to see a wide range that our prediction accuracy can stretch from. Random forests and other bagging methods utilize bootstrap methods in their algorithms to take take subsets of data with replacement. Therefore, we can try bootstraping to get a better understanding of metrics using other models (logistic regression in this case). On depresion, bootstrapping with 1000 replications gives us an accuracy of 0.8, bias of 0.02677731, and std. error of 0.07158341. Looking at the distribution of prediction accuarcies, it seems to follow a normal distribution where predictions of about 0.8 had the highest frequency. There are few outliers with acuracies lower than 0.6 and others that were near 1. The data given gives low prediction accuracy for anxiety, possibly due to the difficulty of predicting thir outcome with the survey data we have. Accuracy rate is 0.5666667 with bias of 0.06373458 and std. error of 0.09111097. The 95 % CI is (0.3333, 0.6774).

## Tree Models

Using bootstrapping in boosted trees, depression predictions have 0.7808617 accuracy and ROC AUC = 0.7792286, which tells us that the model was able to use the data to distinguish between binary classes with moderate strength. Once again, anxiety has proven to be difficult t o predict with a 0.5558013 accuracy and ROC AUC = 0.4604968, indicating that the model can not distinguish between binary classes with our data.

## Final Remarks

While more data trains a stronger model, some situations make it difficult to collect usable and reliable data. Reusing the data over and over again for collecting accurate metrics almost sounds too good to be true. By utilizing bootstrap resampling on student-collected data and making little assumptions about the distribution, we have proven that depression can be predicted with a moderately strong accuracy. Even though we did set out to create this vignette to demonstrate the power of bootstrapping to aspiring Data Scientists, perhaps what is even better is the illustration of applying these methods towards such a passionate topic. An application like this could benefit students in their college experience, parents wanting their kids to feel happier, and Universities to understand what changes need to happen at their school. We all just want to feel happy in life, and it seems like bootstrap is the cure!

![](images/paste-D819E233.png)

[Image 3 Source](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.zevohealth.com%2Fblog%2F6-ways-to-improve-employee-mental-health%2F&psig=AOvVaw0wxwIWH-GtIxCMO3lgskld&ust=1670464914696000&source=images&cd=vfe&ved=0CA8QjRxqFwoTCIiK2e-05vsCFQAAAAAdAAAAABAE)
